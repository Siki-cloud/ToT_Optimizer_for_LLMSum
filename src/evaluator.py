# src/evaluator.py (ä¿®å¤ç‰ˆæœ¬)
import asyncio
import json
import random
import hashlib
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
import re
from tqdm import tqdm

import numpy as np
from rouge_score import rouge_scorer
from bert_score import BERTScorer


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    faithfulness: float = 0.0  # å¿ å®åº¦
    conciseness: float = 0.0   # ç®€æ´æ€§
    completeness: float = 0.0  # å®Œæ•´æ€§
    readability: float = 0.0   # å¯è¯»æ€§
    insightfulness: float = 0.0 # æ´å¯ŸåŠ›
    overall: float = 0.0       # æ€»ä½“è¯„åˆ†
    
    def to_dict(self) -> Dict[str, float]:
        return {
            "faithfulness": self.faithfulness,
            "conciseness": self.conciseness,
            "completeness": self.completeness,
            "readability": self.readability,
            "insightfulness": self.insightfulness,
            "overall": self.overall
        }

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    prompt: str
    metrics: EvaluationMetrics
    summaries: List[str]
    paper_ids: List[str]
    details: Dict[str, Any]
    cache_key: str = ""
    
    @property
    def score(self) -> float:
        """è·å–åŠ æƒæ€»åˆ†"""
        weights = {
            "faithfulness": 0.25,
            "conciseness": 0.20,
            "completeness": 0.25,
            "readability": 0.15,
            "insightfulness": 0.15
        }
        
        total = 0.0
        metrics_dict = self.metrics.to_dict()
        for metric, weight in weights.items():
            if metric != "overall":
                total += metrics_dict.get(metric, 0.0) * weight
        
        return total

class PaperData:
    """è®ºæ–‡æ•°æ®ç±»"""
    
    def __init__(self, paper_id: str, content: str, domain: str, title:str,
                 key_points: List[str] = None, gold_summary: str = None):
        self.id = paper_id
        self.title = title
        self.content = content
        self.domain = domain
        self.key_points = key_points or []
        self.gold_summary = gold_summary
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "title":self.title,
            "content": self.content,
            "domain": self.domain,
            "key_points": self.key_points,
            "gold_summary": self.gold_summary
        }

class PromptEvaluator:
    """æç¤ºè¯„ä¼°å™¨"""
    
    def __init__(self, target_llm, evaluator_llm, papers: List[PaperData], 
                 config: Dict[str, Any]):
        self.target_llm = target_llm
        self.evaluator_llm = evaluator_llm
        self.papers = papers
        self.config = config
        
        # ç¼“å­˜
        self.cache = {}
        self.cache_hits = 0
        self.total_evaluations = 0
        
        # æŒ‰é¢†åŸŸåˆ†ç»„
        self.domain_groups = {}
        for paper in papers:
            domain = paper.domain
            if domain not in self.domain_groups:
                self.domain_groups[domain] = []
            self.domain_groups[domain].append(paper)
    
    def _get_cache_key(self, prompt: str, paper_ids: List[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        papers_hash = hashlib.md5("|".join(sorted(paper_ids)).encode()).hexdigest()
        return f"{prompt_hash}_{papers_hash}"
    
    def _sample_papers(self, num_samples: int) -> List[PaperData]:
        """é‡‡æ ·è®ºæ–‡ï¼ˆç¡®ä¿é¢†åŸŸå¤šæ ·æ€§ï¼‰"""
        if num_samples >= len(self.papers) or num_samples < 0:
            return self.papers[:]
        
        # åˆ†å±‚é‡‡æ ·
        selected = []
        domains = list(self.domain_groups.keys())
        
        # ç¡®ä¿æ¯ä¸ªé¢†åŸŸè‡³å°‘æœ‰ä¸€ç¯‡
        papers_per_domain = max(1, num_samples // len(domains))
        
        for domain in domains:
            if domain in self.domain_groups:
                papers = self.domain_groups[domain]
                if len(papers) <= papers_per_domain:
                    selected.extend(papers)
                else:
                    selected.extend(random.sample(papers, papers_per_domain))
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œéšæœºè¡¥å……
        if len(selected) < num_samples:
            selected_ids = {p.id for p in selected}
            available = [p for p in self.papers if p.id not in selected_ids]
            if available:
                additional = random.sample(available, min(num_samples - len(selected), len(available)))
                selected.extend(additional)
        
        return selected[:num_samples]
    
    async def evaluate_prompt(self, prompt: str, num_samples: int = None, 
                            use_cache: bool = True) -> EvaluationResult:
        """
        è¯„ä¼°æç¤º
        
        Args:
            prompt: è¦è¯„ä¼°çš„æç¤º
            num_samples: é‡‡æ ·è®ºæ–‡æ•°é‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        if num_samples is None:
            num_samples = self.config.get("samples_per_eval", 3)
        
        # é‡‡æ ·è®ºæ–‡
        sampled_papers = self._sample_papers(num_samples)
        paper_ids = [p.id for p in sampled_papers]
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(prompt, paper_ids)
        if use_cache and cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]
        
        self.total_evaluations += 1
        
        # å¹¶è¡Œè¯„ä¼°æ¯ç¯‡è®ºæ–‡
        tasks = []
        for paper in sampled_papers:
            task = self._evaluate_single(prompt, paper)
            tasks.append(task)
        
        paper_results = await asyncio.gather(*tasks)
        
        # èšåˆç»“æœ
        result = self._aggregate_results(prompt, paper_results, sampled_papers, cache_key)
        
        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = result
        
        return result
    
    async def _evaluate_single(self, prompt: str, paper: PaperData) -> Dict[str, Any]:
        """è¯„ä¼°å•ç¯‡è®ºæ–‡"""
        try:
            # 1. ç”Ÿæˆæ‘˜è¦
            # ä¿®å¤è¿™é‡Œï¼šä½¿ç”¨åˆ‡ç‰‡è€Œä¸æ˜¯åœ¨f-stringä¸­ä½¿ç”¨åæ–œæ 
            paper_content_preview = paper.content[:1500]
            full_prompt = f"{prompt}\n\nè®ºæ–‡å†…å®¹ï¼š\n{paper_content_preview}"
            
            summary_response = await self.target_llm.generate(
                [full_prompt],
                generation_config=None  # ä½¿ç”¨é»˜è®¤é…ç½®
            )
            
            summary = summary_response[0] if summary_response else ""
            
            # 2. è®¡ç®—æŒ‡æ ‡
            metrics = await self._compute_metrics(paper, summary)
            
            return {
                "paper_id": paper.id,
                "domain": paper.domain,
                "summary": summary,
                "metrics": metrics.to_dict(),
                "success": True
            }
            
        except Exception as e:
            print(f"è¯„ä¼°è®ºæ–‡ {paper.id} æ—¶å‡ºé”™: {e}")
            return {
                "paper_id": paper.id,
                "domain": paper.domain,
                "summary": "",
                "metrics": EvaluationMetrics().to_dict(),
                "success": False
            }
    
    async def _compute_metrics(self, paper: PaperData, summary: str) -> EvaluationMetrics:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = EvaluationMetrics()
        
        # 1. è§„åˆ™æŒ‡æ ‡ï¼ˆå¿«é€Ÿï¼‰
        metrics.conciseness = self._compute_conciseness(summary)
        metrics.completeness = self._compute_completeness(summary, paper.key_points)
        metrics.readability = self._compute_readability(summary)
        
        # 2. LLMæŒ‡æ ‡ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰
        llm_metrics = await self._compute_llm_metrics(paper.content, summary, paper.gold_summary)
        metrics.faithfulness = llm_metrics.get("faithfulness", 0.5)
        metrics.insightfulness = llm_metrics.get("insightfulness", 0.5)
        
        # 3. è®¡ç®—æ€»ä½“è¯„åˆ†
        weights = self.config.get("metric_weights", {
            "faithfulness": 0.25, "conciseness": 0.20, "completeness": 0.25,
            "readability": 0.15, "insightfulness": 0.15
        })
        
        total = 0.0
        metrics_dict = metrics.to_dict()
        for metric, weight in weights.items():
            if metric != "overall":
                total += metrics_dict.get(metric, 0.0) * weight
        
        metrics.overall = total
        
        return metrics
    
    def _compute_conciseness(self, summary: str) -> float:
        """è®¡ç®—ç®€æ´æ€§"""
        if not summary:
            return 0.0
        
        word_count = len(summary.split())
        
        # ç†æƒ³é•¿åº¦ï¼š100-200è¯
        if word_count <= 100:
            return 0.9  # éå¸¸ç®€æ´
        elif word_count <= 150:
            return 0.8  # ç®€æ´
        elif word_count <= 200:
            return 0.7  # é€‚ä¸­
        elif word_count <= 250:
            return 0.5  # ç¨é•¿
        elif word_count <= 300:
            return 0.3  # è¿‡é•¿
        else:
            return 0.1  # éå¸¸é•¿
    
    def _compute_completeness(self, summary: str, key_points: List[str]) -> float:
        """è®¡ç®—å®Œæ•´æ€§ï¼ˆå…³é”®ç‚¹è¦†ç›–ï¼‰"""
        if not summary or not key_points:
            return 0.5
        
        summary_lower = summary.lower()
        covered = 0
        
        for point in key_points:
            # æ£€æŸ¥å…³é”®ç‚¹ä¸­çš„å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æ‘˜è¦ä¸­
            keywords = point.lower().split()[:3]
            if any(keyword in summary_lower for keyword in keywords if len(keyword) > 3):
                covered += 1
        
        return covered / len(key_points)
    
    def _compute_readability(self, summary: str) -> float:
        """è®¡ç®—å¯è¯»æ€§"""
        if not summary:
            return 0.0
        
        # ç®€å•å¯å‘å¼ï¼šå¥å­æ•°é‡ã€å¹³å‡å¥é•¿
        sentences = [s.strip() for s in summary.split('.') if s.strip()]
        if len(sentences) == 0:
            return 0.5
        
        word_count = len(summary.split())
        avg_sentence_len = word_count / len(sentences)
        
        # ç†æƒ³å¹³å‡å¥é•¿ï¼š15-25è¯
        if 15 <= avg_sentence_len <= 25:
            return 0.8
        elif 10 <= avg_sentence_len < 15 or 25 < avg_sentence_len <= 30:
            return 0.6
        elif 5 <= avg_sentence_len < 10 or 30 < avg_sentence_len <= 40:
            return 0.4
        else:
            return 0.2
    
    async def _compute_llm_metrics(self, paper_content: str, summary: str, 
                              gold_summary: str = None) -> Dict[str, float]:
        
        """ä½¿ç”¨LLMè®¡ç®—æŒ‡æ ‡ - è‹±æ–‡ç‰ˆæœ¬"""
        # å¯¼å…¥è‹±æ–‡æç¤ºæ„å»ºå™¨
        from prompt_builder import PromptBuilder
        
        eval_prompt = PromptBuilder.build_evaluation_prompt(
            paper_content=paper_content,
            summary=summary,
            gold_summary=gold_summary
        )
        # åˆ›å»ºGenerationConfigå¯¹è±¡
        from vllm_server import GenerationConfig  # ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„ç±»
        
        generation_config = GenerationConfig(
            temperature=0.1, # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®š
            max_tokens=150,# é™åˆ¶é•¿åº¦
            stop=["\n\n", "##", "explain"]  # æå‰åœæ­¢è¯
            # stopå‚æ•°å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå¦‚æœGenerationConfigä¸æ”¯æŒstopå‚æ•°
        )
        
        try:
            # è®¾ç½®æ›´ä¸¥æ ¼çš„ç”Ÿæˆå‚æ•°
            response = await self.evaluator_llm.generate(
                [eval_prompt],
                generation_config=generation_config
            )
            
            response_text = response[0] if response else ""
            
            # è°ƒè¯•æ—¥å¿—
            # print(f"=== LLMè¯„ä¼°å“åº” ===")
            # print(f"é•¿åº¦: {len(response_text)}")
            # print(f"å†…å®¹: {response_text[:200]}")
            # print("=" * 40)
            
            # å¤šç­–ç•¥è§£æ
            parsed_data = None
            
            # ç­–ç•¥1ï¼šç›´æ¥JSONè§£æ
            try:
                parsed_data = json.loads(response_text.strip())
            except json.JSONDecodeError:
                pass
            
            # ç­–ç•¥2ï¼šæå–JSONå—
            if not parsed_data:
                parsed_data = self._extract_json_from_text(response_text)
            
            # ç­–ç•¥3ï¼šæ­£åˆ™æå–æ•°å­—
            if not parsed_data:
                parsed_data = self._extract_scores_with_regex(response_text)
            
            # ç­–ç•¥4ï¼šä½¿ç”¨å¤‡ç”¨è¯„åˆ†
            if not parsed_data:
                parsed_data = self._compute_fallback_scores(paper_content, summary)
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
            faithfulness = self._normalize_score(parsed_data.get("faithfulness", 
                                                            parsed_data.get("f", 5.0)))
            insightfulness = self._normalize_score(parsed_data.get("insightfulness", 
                                                                parsed_data.get("i", 5.0)))
            
            return {
                "faithfulness": faithfulness / 10.0,
                "insightfulness": insightfulness / 10.0
            }
            
        except Exception as e:
            print(f"LLMæŒ‡æ ‡è®¡ç®—é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {"faithfulness": 0.5, "insightfulness": 0.5}

    def _extract_json_from_text(self, text: str) -> Dict:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        import re
        
        # åŒ¹é…JSONå¯¹è±¡
        json_pattern = r'\{[^{}]*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        for match in matches:
            try:
                # å°è¯•æ¸…ç†å’Œè§£æ
                cleaned = match.strip()
                # ç¡®ä¿é”®æœ‰å¼•å·
                if '"' not in cleaned and "'" not in cleaned:
                    # å°è¯•æ·»åŠ å¼•å·
                    cleaned = re.sub(r'(\w+):', r'"\1":', cleaned)
                
                data = json.loads(cleaned)
                if isinstance(data, dict):
                    return data
            except:
                continue
        
        return {}

    def _extract_scores_with_regex(self, text: str) -> Dict:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åˆ†æ•°"""
        import re
        
        # åŒ¹é…åˆ†æ•°æ¨¡å¼
        patterns = [
            r'"faithfulness"\s*:\s*(\d+(?:\.\d+)?)',
            r'"f"\s*:\s*(\d+(?:\.\d+)?)',
            r'faithfulness[:\s]+(\d+(?:\.\d+)?)',
            r'å¿ å®åº¦[:\s]+(\d+(?:\.\d+)?)',
        ]
        
        faithfulness_score = None
        insightfulness_score = None
        
        # æå–å¿ å®åº¦
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    faithfulness_score = float(match.group(1))
                    break
                except:
                    continue
        
        # æå–æ´å¯ŸåŠ›
        insight_patterns = [
            r'"insightfulness"\s*:\s*(\d+(?:\.\d+)?)',
            r'"i"\s*:\s*(\d+(?:\.\d+)?)',
            r'insightfulness[:\s]+(\d+(?:\.\d+)?)',
            r'æ´å¯ŸåŠ›[:\s]+(\d+(?:\.\d+)?)',
        ]
        
        for pattern in insight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    insightfulness_score = float(match.group(1))
                    break
                except:
                    continue
        
        result = {}
        if faithfulness_score is not None:
            result["faithfulness"] = faithfulness_score
        if insightfulness_score is not None:
            result["insightfulness"] = insightfulness_score
        
        return result

    def _normalize_score(self, score) -> float:
        """æ ‡å‡†åŒ–åˆ†æ•°åˆ°0-10èŒƒå›´"""
        try:
            score = float(score)
            if score > 100:  # å¯èƒ½æ˜¯ç™¾åˆ†æ¯”
                score = score / 10.0
            elif score > 10:  # å¯èƒ½æ˜¯0-100
                score = score / 10.0
            elif score > 1 and score <= 5:  # å¯èƒ½æ˜¯1-5åˆ†åˆ¶
                score = score * 2
            elif score <= 1:  # å¯èƒ½æ˜¯0-1
                score = score * 10
            
            return max(0, min(10, score))
        except:
            return 5.0

    def _compute_fallback_scores(self, paper_content: str, summary: str) -> Dict:
        """å¯å‘å¼è¯„åˆ†ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        # åŸºäºæ–‡æœ¬ç›¸ä¼¼æ€§ç®€å•è¯„åˆ†
        import difflib
        
        content_words = set(paper_content[:300].lower().split())
        summary_words = set(summary.lower().split())
        
        # Jaccardç›¸ä¼¼åº¦
        if content_words and summary_words:
            intersection = content_words & summary_words
            union = content_words | summary_words
            similarity = len(intersection) / len(union) if union else 0
        else:
            similarity = 0.5
        
        # è½¬æ¢åˆ°0-10åˆ†
        faithfulness_score = min(10, similarity * 12)
        insightfulness_score = 7.0  # é»˜è®¤ä¸­ç­‰
        
        return {'faithfulness': faithfulness_score, 'insightfulness': insightfulness_score}
 
    def _aggregate_results(self, prompt: str, paper_results: List[Dict], 
                          papers: List[PaperData], cache_key: str) -> EvaluationResult:
        """èšåˆå¤šä¸ªè®ºæ–‡çš„ç»“æœ"""
        successful = [r for r in paper_results if r["success"]]
        
        if not successful:
            return EvaluationResult(
                prompt=prompt,
                metrics=EvaluationMetrics(),
                summaries=[],
                paper_ids=[p.id for p in papers],
                details={"error": "æ‰€æœ‰è¯„ä¼°éƒ½å¤±è´¥"},
                cache_key=cache_key
            )
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metrics_sum = {k: 0.0 for k in EvaluationMetrics().__dict__.keys()}
        count = 0
        
        for result in successful:
            for metric, value in result["metrics"].items():
                if metric in metrics_sum:
                    metrics_sum[metric] += value
            count += 1
        
        avg_metrics = EvaluationMetrics()
        for metric in metrics_sum:
            if count > 0:
                setattr(avg_metrics, metric, metrics_sum[metric] / count)
        
        # æ”¶é›†æ‘˜è¦
        summaries = [r["summary"] for r in successful if r.get("summary")]
        
        # è®¡ç®—é¢†åŸŸåˆ†å¸ƒ
        domain_counts = {}
        for result in successful:
            domain = result.get("domain", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
        
        return EvaluationResult(
            prompt=prompt,
            metrics=avg_metrics,
            summaries=summaries,  # æœ€å¤šä¿ç•™ä¸ªæ‘˜è¦
            paper_ids=[p.id for p in papers],
            details={
                "total_papers": len(papers),
                "successful_evaluations": len(successful),
                "domain_distribution": domain_counts,
                "cache_key": cache_key
            },
            cache_key=cache_key
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_papers": len(self.papers),
            "domains": list(self.domain_groups.keys()),
            "cache_size": len(self.cache),
            "cache_hits": self.cache_hits,
            "total_evaluations": self.total_evaluations
        }
    



class SummaryEvaluator:
    """æ‘˜è¦è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, model_type=None, language='en'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        """
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], 
            use_stemmer=True
        )
        
        # åˆå§‹åŒ–BERTScorer
        if not model_type:
            model_type = "bert-base-multilingual-cased"
            # model_type = "/mnt/sharedata/ssd_large/common/SLMs/bert-base-multilingual-cased"
        self.bert_scorer = BERTScorer(
            model_type=model_type,
            lang=language,
            rescale_with_baseline=True,
            device='cpu'
        )

        self.summary_metrics=None
    
    def compute_metrics(self, generated_summary: str, gold_summary: str) -> Dict[str, float]:
        """
        è®¡ç®—å•ä¸ªæ‘˜è¦å¯¹çš„æŒ‡æ ‡
        """
        metrics = {}
        
        # 1. è®¡ç®—ROUGEåˆ†æ•°
        rouge_scores = self.rouge_scorer.score(gold_summary, generated_summary)
        
        metrics['rouge1'] = rouge_scores['rouge1'].fmeasure
        metrics['rouge2'] = rouge_scores['rouge2'].fmeasure
        metrics['rougeL'] = rouge_scores['rougeL'].fmeasure
        
        # 2. è®¡ç®—BERTScore
        P, R, F1 = self.bert_scorer.score([generated_summary], [gold_summary])
        metrics['bertscore_f1'] = float(F1[0])
        metrics['bertscore_precision'] = float(P[0])
        metrics['bertscore_recall'] = float(R[0])
        
        # 3. è®¡ç®—ç»¼åˆåˆ†æ•°
        metrics['composite_score'] = 0.5 * metrics['rouge2'] + 0.5 * metrics['bertscore_f1']
        
        return metrics
    
    def compute_batch_metrics(self, generated_summaries: List[str], 
                            gold_summaries: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡è®¡ç®—æŒ‡æ ‡å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯
        """
        if len(generated_summaries) != len(gold_summaries):
            raise ValueError("ç”Ÿæˆçš„æ‘˜è¦å’Œå‚è€ƒæ‘˜è¦æ•°é‡å¿…é¡»ç›¸åŒ")
        
        all_metrics = []
        
        for gen, gold in zip(generated_summaries, gold_summaries):
            metrics = self.compute_metrics(gen, gold)
            all_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {}
        for key in all_metrics[0].keys():
            values = [m[key] for m in all_metrics]
            avg_metrics[f'avg_{key}'] = np.mean(values)
            avg_metrics[f'std_{key}'] = np.std(values)
            avg_metrics[f'min_{key}'] = np.min(values)
            avg_metrics[f'max_{key}'] = np.max(values)
        
        summary_metrics = {
            'per_sample': all_metrics,
            'statistics': avg_metrics,
        }
        self.summary_metrics = summary_metrics
        return summary_metrics
    
    def print_result(self):
        if self.summary_metrics is not None:
            # æ‰“å°ç»“æœ
            stats = self.summary_metrics['statistics']
                
            # print(f"\nğŸ“Š è¯„ä¼°æ ·æœ¬æ•°: {min_len}")
            print("\nğŸ“ˆ æ‘˜è¦è´¨é‡æŒ‡æ ‡:")
            print(f"{'æŒ‡æ ‡':<20} {'å¹³å‡å€¼':<10} {'æ ‡å‡†å·®':<10} {'èŒƒå›´':<15}")
            print("-" * 55)
            
            metrics_to_show = {
                    'rouge2': 'ROUGE-2 F1',
                    'rougeL': 'ROUGE-L F1',
                    'bertscore_f1': 'BERTScore F1',
                    'composite_score': 'ç»¼åˆåˆ†æ•°'
                }
                
            for key, display_name in metrics_to_show.items():
                    avg = stats[f'avg_{key}']
                    std = stats[f'std_{key}']
                    min_val = stats[f'min_{key}']
                    max_val = stats[f'max_{key}']
                    print(f"{display_name:<20} {avg:<10.4f} {std:<10.4f} [{min_val:.4f}-{max_val:.4f}]")
                
