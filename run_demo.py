# run_demo.py
#!/usr/bin/env python3
"""
ToT-PromptOptimizer æ¼”ç¤ºè„šæœ¬
"""

import asyncio
import json
import yaml
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.append(str(current_dir / "src"))

from vllm_server import VLLMServer
from evaluator import PromptEvaluator, PaperData, SummaryEvaluator
from tot_optimizer import ToTPromptOptimizer
from component_lib import ComponentLibrary
from search_visualizer import visualize_search_tree
from prompt_builder import PromptBuilder

class ToTPromptOptimizerDemo:
    """ToTæç¤ºä¼˜åŒ–å™¨æ¼”ç¤ºç±»"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        self.config_path = config_path
        self.config = self.load_config()
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path(self.config["data"]["results_dir"])
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹å®ä¾‹
        self.target_llm = None
        self.evaluator_llm = None
        
        # æ•°æ®
        self.train_data = []
        self.val_data = []
        self.test_data = []
        
        # ç»„ä»¶åº“
        self.components = []
        
        # ä¼˜åŒ–å™¨å®ä¾‹
        self.optimizer = None
        
    def load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    
    async def initialize_models(self):
        """åˆå§‹åŒ–LLMæ¨¡å‹"""
        print("=" * 60)
        print("åˆå§‹åŒ–LLMæ¨¡å‹")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç›®æ ‡LLM
        print(f"\n1. åˆå§‹åŒ–ç›®æ ‡LLM...")
        target_config = self.config["models"]["target_model"]
        self.target_llm = VLLMServer(target_config)
        await self.target_llm.initialize()
        
        # åˆå§‹åŒ–è¯„ä¼°LLM
        print(f"\n2. åˆå§‹åŒ–è¯„ä¼°LLM...")
        evaluator_config = self.config["models"]["evaluator_model"]
        self.evaluator_llm = VLLMServer(evaluator_config)
        await self.evaluator_llm.initialize()
        
        print(f"\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ ‡LLM: {target_config['path']} on {target_config['device']}")
        print(f"   è¯„ä¼°LLM: {evaluator_config['path']} on {evaluator_config['device']}")
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\n" + "=" * 60)
        print("åŠ è½½æ•°æ®é›†")
        print("=" * 60)
        
        data_dir = Path(self.config["data"].get("cache_dir", "./data"))
        train_file_name = self.config["data"].get("train_file", "train_papers.json")
        test_file_name = self.config["data"].get("test_file", "test_papers.json")
        val_file_name = self.config["data"].get("val_file", "val_papers.json")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_file = data_dir / train_file_name
        if train_file.exists():
            with open(train_file, 'r', encoding='utf-8') as f:
                train_dicts = json.load(f)
            self.train_data = [PaperData(**paper) for paper in train_dicts]
            print(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {len(self.train_data)} ç¯‡è®ºæ–‡")
        else:
            print(f"âš ï¸  è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            self.generate_mock_data()
        
        # åŠ è½½éªŒè¯æ•°æ®
        val_file = data_dir / test_file_name
        if not self.val_data and  val_file.exists():
            with open(val_file, 'r', encoding='utf-8') as f:
                val_dicts = json.load(f)
            self.val_data = [PaperData(**paper) for paper in val_dicts]
            print(f"âœ… åŠ è½½éªŒè¯æ•°æ®: {len(self.val_data)} ç¯‡è®ºæ–‡")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_file = data_dir / val_file_name
        if not self.test_data and test_file.exists():
            with open(test_file, 'r', encoding='utf-8') as f:
                test_dicts = json.load(f)
            self.test_data = [PaperData(**paper) for paper in test_dicts]
            print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {len(self.test_data)} ç¯‡è®ºæ–‡")
    
    def generate_mock_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        print("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        domains = ["Computer Vision", "NLP", "Reinforcement Learning"]
        train_size = int(self.config["data"].get("train_size", 50))
        val_size = int(self.config["data"].get("val_size", 15))
        test_size = int(self.config["data"].get("test_size", 10))

        papers = []
        for i in range(train_size+val_size+test_size):
            paper = PaperData(
                paper_id=f"demo_paper_{i:03d}",
                title=f"Deep Learning Approach for {domains[i % len(domains)]}",
                content=f"This is a demo paper about {domains[i % len(domains)]}. It proposes a novel method with significant improvements.",
                domain=domains[i % len(domains)],
                key_points=["novel method", "experimental validation", "performance improvement"],
                gold_summary=f"Demo paper {i} summary in {domains[i % len(domains)]}."
            )
            papers.append(paper)
        
            
        self.train_data = papers[:train_size]
        self.test_data = papers[train_size:train_size+test_size]
        self.val_data = papers[train_size+test_size:]
        
        print(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®: {len(self.train_data)} ç¯‡è®ºæ–‡;éªŒè¯æ•°æ®ï¼š{len(self.val_data)};æµ‹è¯•æ•°æ®ï¼š{len(self.test_data)}")

    def load_components(self):
        """åŠ è½½ç»„ä»¶åº“"""
        print("\n" + "=" * 60)
        print("åŠ è½½æç¤ºç»„ä»¶åº“")
        print("=" * 60)
        
        data_dir = Path(self.config["data"].get("cache_dir", "./data"))
        component_file_name = self.config["data"].get("components_file", "component_library.json")
        
        component_file = data_dir / component_file_name
        
        if component_file.exists():
            self.components = ComponentLibrary.load_from_file(str(component_file))
        else:
            self.components = ComponentLibrary.load_default()
            # ä¿å­˜é»˜è®¤ç»„ä»¶åº“
            ComponentLibrary.save_to_file(self.components, str(component_file))
        
        print(f"âœ… åŠ è½½ç»„ä»¶: {len(self.components)} ä¸ª")
        
        # åˆ†æç»„ä»¶åº“
        analysis = ComponentLibrary.analyze_components(self.components)
        print(f"   å†²çªå…³ç³»: {len(analysis['conflict_graph'])} ç»„")
        print(f"   ä¾èµ–å…³ç³»: {len(analysis['dependency_graph'])} ç»„")
        
        # æ˜¾ç¤ºä¸€äº›ç»„ä»¶
        print(f"\nç¤ºä¾‹ç»„ä»¶:")
        for i, comp in enumerate(self.components[:3]):
            print(f"  {i+1}. {comp['id']}: {comp['text']}")
    
    async def create_evaluator(self, split: str = "train") -> PromptEvaluator:
        """åˆ›å»ºè¯„ä¼°å™¨"""
        if split == "train":
            papers = self.train_data
        elif split == "val":
            papers = self.val_data
        elif split == "test":
            papers = self.test_data
        else:
            papers = self.train_data
        
        evaluator = PromptEvaluator(
            target_llm=self.target_llm,
            evaluator_llm=self.evaluator_llm,
            papers=papers,
            config=self.config["evaluation"]
        )
        
        return evaluator
    
    async def run_optimization(self):
        """è¿è¡Œä¼˜åŒ–"""
        print("\n" + "=" * 60)
        print("å¼€å§‹Tree of Thoughtsä¼˜åŒ–")
        print("=" * 60)
        
        # åˆ›å»ºè¯„ä¼°å™¨
        train_evaluator = await self.create_evaluator("train")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = ToTPromptOptimizer(
            components=self.components,
            evaluator=train_evaluator,
            config=self.config["optimization"]
        )
        
        # è¿è¡Œä¼˜åŒ–
        await self.optimizer.optimize(
            iterations=self.config["optimization"]["max_iterations"],
            search_method="mcts"  # æˆ– "beam"
        )
        
        # ä¿å­˜ç»“æœ
        results_file = self.results_dir / "optimization_results.json"
        self.optimizer.save_results(str(results_file))
        
        # å¯è§†åŒ–æœç´¢æ ‘
        await self.visualize_results()
    
    async def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        print("\n" + "=" * 60)
        print("å¯è§†åŒ–ç»“æœ")
        print("=" * 60)
        
        # è·å–æœ€ä½³æç¤º
        best_prompt, best_details = self.optimizer.get_best_prompt()
        
        print(f"\nğŸ¯ æœ€ä½³æç¤º:")
        print(f"   {best_prompt}")
        
        print(f"\nğŸ“Š è¯„ä¼°åˆ†æ•°: {best_details.get('score', 0):.3f}")
        
        if "metrics" in best_details:
            print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
            metrics = best_details["metrics"]
            for metric, score in metrics.items():
                print(f"   {metric}: {score:.3f}")
        
        # æ˜¾ç¤ºç¤ºä¾‹æ‘˜è¦
        if "summaries" in best_details and best_details["summaries"]:
            print(f"\nğŸ“ ç¤ºä¾‹ç”Ÿæˆçš„æ‘˜è¦:")
            print(f"   {best_details['summaries'][0][:200]}...")
        
        # å¯è§†åŒ–æœç´¢æ ‘
        try:
            tree_data = self.optimizer.get_search_tree()
            
            # ä¿å­˜å¯è§†åŒ–
            vis_file = self.results_dir / "search_tree.html"
            visualize_search_tree(tree_data, str(vis_file))
            print(f"\nğŸŒ³ æœç´¢æ ‘å¯è§†åŒ–å·²ä¿å­˜åˆ°: {vis_file}")
            
        except ImportError:
            print("\nâš ï¸  æœç´¢æ ‘å¯è§†åŒ–éœ€è¦é¢å¤–çš„ä¾èµ–")
        
        # ç»˜åˆ¶ä¼˜åŒ–æ›²çº¿
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            history = self.optimizer.search_history
            if history:
                iterations = [h["iteration"] for h in history]
                scores = [h["best_score"] for h in history]
                
                plt.figure(figsize=(10, 6))
                plt.plot(iterations, scores, 'b-', linewidth=2, marker='o', markersize=4)
                plt.xlabel(' Iteration Time')
                plt.ylabel('Best Score')
                plt.title('ToT Optimization Progress')
                plt.grid(True, alpha=0.3)
                
                # ä¿å­˜å›¾è¡¨
                chart_file = self.results_dir / "optimization_progress.png"
                plt.savefig(str(chart_file), dpi=150, bbox_inches='tight')
                print(f"ğŸ“ˆ ä¼˜åŒ–è¿›åº¦å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
                
                plt.close()
        
        except ImportError:
            print("âš ï¸  å›¾è¡¨ç»˜åˆ¶éœ€è¦matplotlib")
    

    async def evaluate_on_test_set(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æç¤º"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°")
        print("=" * 60)
        
        if not self.optimizer:
            print("âŒ è¯·å…ˆè¿è¡Œä¼˜åŒ–")
            return
        
        # è·å–æœ€ä½³æç¤º
        best_prompt, _ = self.optimizer.get_best_prompt()
        
        # åˆ›å»ºæµ‹è¯•é›†è¯„ä¼°å™¨
        test_evaluator = await self.create_evaluator("test")
        
        print(f"\nè¯„ä¼°æç¤º: {best_prompt}")
        
        # è¯„ä¼°
        test_result = await test_evaluator.evaluate_prompt(
            best_prompt,
            num_samples=min(5, len(self.test_data))
        )
        
        # è·å–æµ‹è¯•é›†ä¸­çš„gold summaries
        paper_ids = test_result.paper_ids
        generated_summaries = test_result.summaries
        gold_summaries = [ ]
        for p_id in paper_ids:
            for i in self.test_data:
                if i.id  == p_id :
                    gold_summaries.append(i.gold_summary)
                    break
         
        # åˆå§‹åŒ–æ‘˜è¦è¯„ä¼°å™¨
        summary_evaluator = SummaryEvaluator()
        summary_metrics = {}
        if len(generated_summaries) == len(gold_summaries):
            # print(f"generated_summaries:{len(generated_summaries)}, {generated_summaries}")
            # print(f"gold_summaries:{len(gold_summaries)}, {gold_summaries}")
            # è®¡ç®—æŒ‡æ ‡
            summary_metrics = summary_evaluator.compute_batch_metrics(
                    generated_summaries, 
                    gold_summaries
            )
            # summary_evaluator.print_result()

        
        print(f"\nğŸ“Š æµ‹è¯•é›†ç»“æœ:")
        print(f"   æ€»ä½“åˆ†æ•°: {test_result.score:.3f}")
        
        print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
        metrics_dict = test_result.metrics.to_dict()
        for metric, score in metrics_dict.items():
            if metric != "overall":
                print(f"   {metric}: {score:.3f}")
        
        print(f"\nğŸ“‹ è¯„ä¼°è®ºæ–‡æ•°: {len(test_result.paper_ids)}")
        print(f"   é¢†åŸŸåˆ†å¸ƒ: {test_result.details.get('domain_distribution', {})}")
        
        summary_evaluator.print_result() ## æ‰“å° summary metric

        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results = {
            "prompt": best_prompt,
            "test_score": test_result.score,
            "metrics": metrics_dict,
            "paper_ids": test_result.paper_ids,
            "details": test_result.details,
            "summary_metrics":summary_metrics['statistics']
        }
        
        test_file = self.results_dir / "test_evaluation.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_file}")
    
    async def compare_with_baseline(self):
        """ä¸åŸºçº¿æç¤ºæ¯”è¾ƒ"""
        print("\n" + "=" * 60)
        print("ä¸åŸºçº¿æç¤ºæ¯”è¾ƒ")
        print("=" * 60)
        
        # åŸºçº¿æç¤º
        baseline_prompts = {
            "simple": "Summary this paper",
            "detailed": "Please provide a detailed summary of the paper's core content, methodology, and contributions.",
            "structured": "Please summarize the paper from four aspects: background, methodology, experiments, and conclusions."
        }
        
        # åˆ›å»ºè¯„ä¼°å™¨
        test_evaluator = await self.create_evaluator("test")
        
        # è·å–ä¼˜åŒ–åçš„æœ€ä½³æç¤º
        if self.optimizer:
            best_prompt, _ = self.optimizer.get_best_prompt()
            baseline_prompts["optimized"] = best_prompt
        
        results = {}
        
        print("\nè¯„ä¼°ä¸åŒæç¤º...")
        for name, prompt in baseline_prompts.items():
            print(f"\nè¯„ä¼°: {name}...")
            result = await test_evaluator.evaluate_prompt(
                prompt,
                num_samples=3
            )
            results[name] = {
                "prompt": prompt,
                "score": result.score,
                "metrics": result.metrics.to_dict()
            }
            
            print(f"  åˆ†æ•°: {result.score:.3f}")
        
        # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
        print("\n" + "=" * 60)
        print("æç¤ºæ¯”è¾ƒç»“æœ")
        print("=" * 60)
        
        print("\næ’å:")
        sorted_results = sorted(results.items(), key=lambda x: x[1]["score"], reverse=True)
        
        for i, (name, data) in enumerate(sorted_results):
            print(f"{i+1}. {name}: {data['score']:.3f}")
            print(f"   æç¤º: {data['prompt'][:80]}...")
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        compare_file = self.results_dir / "prompt_comparison.json"
        with open(compare_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {compare_file}")
    
    async def interactive_demo(self):
        """äº¤äº’å¼æ¼”ç¤º"""
        print("\n" + "=" * 60)
        print("äº¤äº’å¼æ¼”ç¤º")
        print("=" * 60)
        
        if not self.optimizer:
            print("âŒ è¯·å…ˆè¿è¡Œä¼˜åŒ–")
            return
        
        best_prompt, best_details = self.optimizer.get_best_prompt()
        
        print(f"\nå½“å‰æœ€ä½³æç¤º: {best_prompt}")
        print(f"åˆ†æ•°: {best_details.get('score', 0):.3f}")
        
        while True:
            print("\né€‰é¡¹:")
            print("1. æµ‹è¯•æ–°è®ºæ–‡")
            print("2. æŸ¥çœ‹æœç´¢æ ‘")
            print("3. ä¿®æ”¹ç»„ä»¶åº“")
            print("4. è¿è¡Œæ›´å¤šä¼˜åŒ–")
            print("5. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
            
            if choice == "1":
                await self.test_custom_paper(best_prompt)
            elif choice == "2":
                await self.explore_search_tree()
            elif choice == "3":
                self.modify_component_library()
            elif choice == "4":
                more_iterations = int(input("è¾“å…¥é¢å¤–è¿­ä»£æ¬¡æ•°: ") or "10")
                await self.optimizer.optimize(iterations=more_iterations)
                best_prompt, best_details = self.optimizer.get_best_prompt()
                print(f"\næ–°æœ€ä½³æç¤º: {best_prompt}")
                print(f"æ–°åˆ†æ•°: {best_details.get('score', 0):.3f}")
            elif choice == "5":
                break
            else:
                print("æ— æ•ˆé€‰æ‹©")
    
    async def test_custom_paper(self, prompt: str):
        """æµ‹è¯•è‡ªå®šä¹‰è®ºæ–‡"""
        print("\nè¾“å…¥è®ºæ–‡å†…å®¹ï¼ˆè¾“å…¥ENDç»“æŸï¼‰:")
        lines = []
        while True:
            line = input()
            if line.strip() == "END":
                break
            lines.append(line)
        
        paper_content = "\n".join(lines)
        
        if not paper_content:
            paper_content = "This is a paper on deep learning that proposes a new neural network architecture."
        
        full_prompt = f"{prompt}\n\nPaper Contentï¼š\n{paper_content}"
        print(f"\nç”Ÿæˆæ‘˜è¦...")
        summary = await self.target_llm.generate([full_prompt])
        
        if summary:
            print(f"\nğŸ“ ç”Ÿæˆçš„æ‘˜è¦:")
            print(f"{summary[0]}")
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥")
    
    async def explore_search_tree(self):
        """æ¢ç´¢æœç´¢æ ‘"""
        if not self.optimizer:
            print("âŒ ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
            return
        
        tree = self.optimizer.get_search_tree()
        
        print(f"\næœç´¢æ ‘ç»Ÿè®¡:")
        print(f"   æ€»èŠ‚ç‚¹æ•°: {len(tree['nodes'])}")
        print(f"   è¾¹æ•°: {len(tree['edges'])}")
        
        # æ˜¾ç¤ºé«˜åˆ†èŠ‚ç‚¹
        nodes = list(tree["nodes"].items())
        nodes.sort(key=lambda x: x[1]["score"], reverse=True)
        
        print(f"\nTop 5 èŠ‚ç‚¹:")
        for i, (node_id, node_data) in enumerate(nodes[:5]):
            state = node_data["state"]
            state_str = " -> ".join(state) if state else "[ç©º]"
            print(f"{i+1}. {state_str} (åˆ†æ•°: {node_data['score']:.3f})")
    
    def modify_component_library(self):
        """ä¿®æ”¹ç»„ä»¶åº“"""
        print(f"\nå½“å‰ç»„ä»¶æ•°: {len(self.components)}")
        print("1. æ·»åŠ ç»„ä»¶")
        print("2. åˆ é™¤ç»„ä»¶")
        print("3. æŸ¥çœ‹ç»„ä»¶")
        print("4. è¿”å›")
        
        choice = input("é€‰æ‹©: ").strip()
        
        if choice == "1":
            comp_id = input("ç»„ä»¶ID: ").strip()
            comp_text = input("ç»„ä»¶æ–‡æœ¬: ").strip()
            
            new_component = {
                "id": comp_id,
                "text": comp_text,
                "effect_vector": {"conciseness": 0.5, "completeness": 0.5},
                "conflicts": [],
                "requires": [],
                "estimated_tokens": len(comp_text.split())
            }
            
            self.components.append(new_component)
            print(f"âœ… æ·»åŠ ç»„ä»¶: {comp_id}")
        
        elif choice == "2":
            print("ç»„ä»¶åˆ—è¡¨:")
            for i, comp in enumerate(self.components):
                print(f"{i+1}. {comp['id']}: {comp['text']}")
            
            idx = int(input("è¦åˆ é™¤çš„ç»„ä»¶ç¼–å·: ").strip()) - 1
            if 0 <= idx < len(self.components):
                removed = self.components.pop(idx)
                print(f"âœ… åˆ é™¤ç»„ä»¶: {removed['id']}")
        
        elif choice == "3":
            print("\nç»„ä»¶è¯¦æƒ…:")
            for comp in self.components:
                print(f"\n{comp['id']}:")
                print(f"  æ–‡æœ¬: {comp['text']}")
                print(f"  æ•ˆæœ: {comp.get('effect_vector', {})}")
                print(f"  å†²çª: {comp.get('conflicts', [])}")
                print(f"  ä¾èµ–: {comp.get('requires', [])}")
    
    async def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¯åŠ¨ ToT-PromptOptimizer æ¼”ç¤º")
        print("=" * 60)
        
        # 1. åˆå§‹åŒ–
        await self.initialize_models()
        
        # 2. åŠ è½½æ•°æ®
        self.load_data()
        
        # 3. åŠ è½½ç»„ä»¶
        self.load_components()
        
        # 4. è¿è¡Œä¼˜åŒ–
        await self.run_optimization()
        
        # 5. æµ‹è¯•é›†è¯„ä¼°
        await self.evaluate_on_test_set()
        
        # 6. ä¸åŸºçº¿æ¯”è¾ƒ
        await self.compare_with_baseline()
        
        # # 7. äº¤äº’å¼æ¼”ç¤ºï¼ˆå¯é€‰ï¼‰
        # interactive = input("\næ˜¯å¦è¿›å…¥äº¤äº’å¼æ¼”ç¤º? (y/n): ").strip().lower()
        # if interactive == 'y':
        #     await self.interactive_demo()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if self.target_llm:
            target_stats = self.target_llm.get_stats()
            print(f"\nç›®æ ‡LLMç»Ÿè®¡:")
            print(f"   è°ƒç”¨æ¬¡æ•°: {target_stats['call_count']}")
            print(f"   ç”Ÿæˆtokenæ•°: {target_stats['total_tokens']}")
        
        if self.optimizer:
            print(f"\nä¼˜åŒ–ç»Ÿè®¡:")
            stats = self.optimizer.stats
            for key, value in stats.items():
                print(f"   {key}: {value}")

async def main():
    """ä¸»å‡½æ•°"""
    demo = ToTPromptOptimizerDemo("config.yaml")
    
    try:
        await demo.run_full_demo()
    except KeyboardInterrupt:
        print("\n\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # æ£€æŸ¥å¹¶ç”Ÿæˆæ•°æ®
    data_dir = Path("./data")
    if not (data_dir / "train_papers.json").exists():
        print("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        import generate_data
        generate_data.save_data({})  # è¿™ä¼šç”Ÿæˆæ•°æ®
        
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())