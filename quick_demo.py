# quick_demo.py
"""
å¿«é€Ÿæ¼”ç¤ºè„šæœ¬ - ä¸éœ€è¦çœŸå®LLMæ¨¡å‹
"""

import asyncio
import json
from pathlib import Path
import sys

# æ·»åŠ srcç›®å½•
sys.path.append(str(Path(__file__).parent / "src"))

from vllm_server import MockLLM
from evaluator import PromptEvaluator, PaperData
from tot_optimizer import ToTPromptOptimizer
from component_lib import ComponentLibrary

async def quick_demo():
    """å¿«é€Ÿæ¼”ç¤º"""
    print("ğŸš€ ToT-PromptOptimizer å¿«é€Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # ä½¿ç”¨æ¨¡æ‹ŸLLM
    target_llm = MockLLM("target-mock")
    evaluator_llm = MockLLM("evaluator-mock")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    papers = []
    domains = ["CV", "NLP", "RL"]
    
    for i in range(20):
        paper = PaperData(
            paper_id=f"paper_{i:03d}",
            content=f"è¿™æ˜¯{i}å·è®ºæ–‡ï¼Œå…³äº{domains[i % len(domains)]}ã€‚æå‡ºäº†æ–°æ–¹æ³•ï¼Œå®éªŒç»“æœè‰¯å¥½ã€‚",
            domain=domains[i % len(domains)],
            key_points=["åˆ›æ–°æ–¹æ³•", "å®éªŒéªŒè¯", "æ€§èƒ½æå‡"],
            gold_summary=f"è®ºæ–‡{i}çš„æ‘˜è¦ã€‚"
        )
        papers.append(paper)
    
    # åŠ è½½ç»„ä»¶
    components = ComponentLibrary.load_default()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PromptEvaluator(
        target_llm=target_llm,
        evaluator_llm=evaluator_llm,
        papers=papers,
        config={
            "samples_per_eval": 2,
            "metric_weights": {
                "faithfulness": 0.25, "conciseness": 0.20, 
                "completeness": 0.25, "readability": 0.15, 
                "insightfulness": 0.15
            }
        }
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = ToTPromptOptimizer(
        components=components,
        evaluator=evaluator,
        config={
            "max_iterations": 15,
            "beam_width": 2,
            "exploration_weight": 1.41,
            "max_depth": 4,
            "max_tokens": 150,
            "task_description": "æ€»ç»“ä¸€ç¯‡æŠ€æœ¯è®ºæ–‡",
            "train_samples_per_eval": 2
        }
    )
    
    # è¿è¡Œä¼˜åŒ–
    print("\nå¼€å§‹ä¼˜åŒ–...")
    await optimizer.optimize(iterations=10, search_method="beam")
    
    # æ˜¾ç¤ºç»“æœ
    best_prompt, best_details = optimizer.get_best_prompt()
    
    print("\n" + "=" * 60)
    print("ä¼˜åŒ–ç»“æœ")
    print("=" * 60)
    
    print(f"\nğŸ¯ æœ€ä½³æç¤º:")
    print(f"   {best_prompt}")
    
    print(f"\nğŸ“Š åˆ†æ•°: {best_details.get('score', 0):.3f}")
    
    if "metrics" in best_details:
        print(f"\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
        for metric, score in best_details["metrics"].items():
            print(f"   {metric}: {score:.3f}")
    
    print(f"\nğŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
    for key, value in optimizer.stats.items():
        print(f"   {key}: {value}")
    
    # ä¿å­˜ç»“æœ
    results = {
        "best_prompt": best_prompt,
        "best_details": best_details,
        "stats": optimizer.stats
    }
    
    with open("quick_demo_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: quick_demo_results.json")
    print("\næ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(quick_demo())